import json
import time
from typing import List, Dict, Optional
from datetime import datetime
import re

from openai_client import chat_completion
from config import (
    DEFAULT_MODEL, BATCH_SIZE, BATCH_INTERVAL, RETRY_ATTEMPTS, 
    RETRY_INTERVAL, APPROVED_GENRES
)


class BatchProcessor:
    def __init__(self, model: str = None):
        self.model = model or DEFAULT_MODEL
        self.batch_size = BATCH_SIZE
        self.batch_interval = BATCH_INTERVAL
        self.retry_attempts = RETRY_ATTEMPTS
        self.retry_interval = RETRY_INTERVAL
        self.approved_genres = APPROVED_GENRES
    
    def process_newsletter_batches(self, newsletters: List[Dict]) -> List[Dict]:
        """
        Process newsletters in batches with LLM analysis
        
        Args:
            newsletters: List of newsletter candidates
            
        Returns:
            List of processed newsletters with summaries and genres
        """
        print(f"ü§ñ Starting batch processing with {self.model}")
        print(f"üìä Processing {len(newsletters)} newsletters in batches of {self.batch_size}")
        
        processed_newsletters = []
        
        # Process newsletters in batches
        for i in range(0, len(newsletters), self.batch_size):
            batch = newsletters[i:i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            total_batches = (len(newsletters) + self.batch_size - 1) // self.batch_size
            
            print(f"\nüì¶ Processing batch {batch_num}/{total_batches} ({len(batch)} newsletters)")
            
            # Process batch with retry logic
            batch_results = self._process_single_batch(batch, batch_num)
            
            if batch_results:
                processed_newsletters.extend(batch_results)
                print(f"‚úÖ Batch {batch_num} completed: {len(batch_results)} newsletters processed")
            else:
                print(f"‚ùå Batch {batch_num} failed after all retries")
            
            # Rate limiting - wait between batches (except for last batch)
            if i + self.batch_size < len(newsletters):
                print(f"‚è≥ Waiting {self.batch_interval} seconds before next batch...")
                time.sleep(self.batch_interval)
        
        print(f"\n‚úÖ Batch processing complete: {len(processed_newsletters)} newsletters processed")
        return processed_newsletters
    
    def _process_single_batch(self, batch: List[Dict], batch_num: int) -> Optional[List[Dict]]:
        """Process a single batch with retry logic"""
        
        for attempt in range(self.retry_attempts):
            try:
                print(f"üîÑ Batch {batch_num}, attempt {attempt + 1}/{self.retry_attempts}")
                
                # Create LLM prompt for batch
                prompt = self._create_batch_prompt(batch)
                
                # Make LLM call
                response = chat_completion([{"role": "user", "content": prompt}], model=self.model)
                
                # Parse structured response
                batch_results = self._parse_batch_response(response, batch)
                
                if batch_results:
                    return batch_results
                else:
                    print(f"‚ö†Ô∏è Batch {batch_num} attempt {attempt + 1} failed: Invalid response format")
                    
            except Exception as e:
                print(f"‚ùå Batch {batch_num} attempt {attempt + 1} error: {e}")
            
            # Wait before retry (except for last attempt)
            if attempt < self.retry_attempts - 1:
                print(f"‚è≥ Waiting {self.retry_interval} seconds before retry...")
                time.sleep(self.retry_interval)
        
        return None
    
    def _create_batch_prompt(self, batch: List[Dict]) -> str:
        """Create LLM prompt for batch analysis"""
        
        # Prepare newsletter info for LLM
        newsletter_info = []
        for i, newsletter in enumerate(batch):
            # Get cleaned content (first 3000 chars to avoid token limits)
            content = newsletter.get('body', '')[:3000]
            
            newsletter_info.append(f"""
Newsletter {i+1}:
Subject: {newsletter.get('subject', 'No Subject')}
Sender: {newsletter.get('sender', 'No Sender')}
Content: {content}
""")
        
        newsletters_text = "\n".join(newsletter_info)
        approved_genres_text = ", ".join(self.approved_genres)
        
        return f"""
You are an expert newsletter analyst. Analyze these {len(batch)} newsletters and provide structured output.

For each newsletter, provide:
1. A comprehensive summary (less than 1000 words) that captures the main content, key insights, and valuable information
2. A genre classification from the approved list (a single genre for each newsletter)

APPROVED GENRES (use exactly these): {approved_genres_text}

NEWSLETTERS TO ANALYZE:
{newsletters_text}

RESPOND WITH VALID JSON ONLY (no other text):
{{
    "newsletters": [
    {{
        "newsletter_id": 1,
        "summary": "Comprehensive summary of newsletter content...",
        "genre": "Technology"
    }},
    {{
        "newsletter_id": 2,
        "summary": "Another newsletter summary...",
        "genre": "Business"
    }}
    ]
}}

IMPORTANT: 
- newsletter_id must match the newsletter number (1, 2, 3, etc.)
- genre must be exactly one of the approved genres
- summary should be comprehensive and valuable for a weekly digest
- skip any emails that are not newsletters
- respond with valid JSON only
"""
    
    def _parse_batch_response(self, response: str, batch: List[Dict]) -> Optional[List[Dict]]:
        """Parse LLM batch response and return processed newsletters"""
        
        try:
            # Clean response to extract JSON
            json_match = re.search(r'\{[\s\S]*\}', response)
            if not json_match:
                print("‚ùå No JSON found in response")
                return None
            
            json_str = json_match.group(0)
            llm_response = json.loads(json_str)
            
            if 'newsletters' not in llm_response:
                print("‚ùå Invalid response format: missing 'newsletters' key")
                return None
            
            processed_newsletters = []
            
            for newsletter_result in llm_response['newsletters']:
                newsletter_id = newsletter_result.get('newsletter_id', 0)
                summary = newsletter_result.get('summary', '')
                genre = newsletter_result.get('genre', '')
                
                # Validate newsletter_id
                if newsletter_id < 1 or newsletter_id > len(batch):
                    print(f"‚ö†Ô∏è Invalid newsletter_id: {newsletter_id}")
                    continue
                
                # Validate genre
                if genre not in self.approved_genres:
                    print(f"‚ö†Ô∏è Invalid genre '{genre}', defaulting to 'Technology'")
                    genre = 'Technology'
                
                # Get original newsletter (convert to 0-based index)
                original_newsletter = batch[newsletter_id - 1]
                
                # Create processed newsletter
                processed_newsletter = {
                    'original_email': original_newsletter,
                    'summary': summary.strip(),
                    'genre': genre,
                    'sender': original_newsletter.get('sender', ''),
                    'subject': original_newsletter.get('subject', ''),
                    'date': original_newsletter.get('date', ''),
                    'word_count': len(summary.split())
                }
                
                processed_newsletters.append(processed_newsletter)
            
            return processed_newsletters
            
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON parsing error: {e}")
            return None
        except Exception as e:
            print(f"‚ùå Response parsing error: {e}")
            return None


def main():
    """Test the batch processor"""
    # Test with sample newsletters
    test_newsletters = [
        {
            'subject': 'Tech Weekly: AI Breakthroughs',
            'sender': 'tech@example.com',
            'date': '2024-01-15',
            'body': 'This week in AI: OpenAI releases new models, Google announces Gemini updates, and Microsoft integrates AI into Office. The future of work is changing rapidly with these AI advancements.'
        },
        {
            'subject': 'Business Insights: Market Trends',
            'sender': 'business@example.com', 
            'date': '2024-01-15',
            'body': 'Market analysis shows strong growth in tech sector. Key trends include: cloud adoption, remote work tools, and cybersecurity investments. Companies are pivoting to digital-first strategies.'
        }
    ]
    
    processor = BatchProcessor()
    results = processor.process_newsletter_batches(test_newsletters)
    
    print(f"\nüìã Test Results:")
    for i, result in enumerate(results):
        print(f"{i+1}. {result['subject']} -> {result['genre']}")
        print(f"   Summary: {result['summary'][:100]}...")
        print()


if __name__ == "__main__":
    main() 